{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Stub evaluation function that returns a random value\n",
        "def evaluate(features):\n",
        "    return random.uniform(0.25, 0.75) * 100  # returns a percentage value\n",
        "\n",
        "# Forward Selection with detailed trace\n",
        "def forward_selection(all_features):\n",
        "  #Algorithm starts with no selected features & best score of 0\n",
        "    selected_features = []\n",
        "    best_score = 0\n",
        "    improvement = True\n",
        "\n",
        "    print(\"\\nYou have selected Forward Selection\\n\")\n",
        "\n",
        "    # Initial random accuracy with no features\n",
        "    initial_score = evaluate(selected_features)\n",
        "    print(f\"Using no features and “random” evaluation, I get an accuracy of {initial_score:.1f}%\\n\")\n",
        "\n",
        "    iteration = 1\n",
        "    while improvement:\n",
        "        print(f\"Beginning search.\\n\")\n",
        "        improvement = False\n",
        "        best_candidate = None\n",
        "        candidates = []\n",
        "        for feature in all_features:\n",
        "            if feature not in selected_features:\n",
        "                candidate_features = selected_features + [feature]\n",
        "                score = evaluate(candidate_features)\n",
        "                candidates.append((feature, score))\n",
        "\n",
        "        for feature, score in candidates:\n",
        "            print(f\"Using feature(s) {selected_features + [feature]} accuracy is {score:.1f}%\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_candidate = feature\n",
        "                improvement = True\n",
        "\n",
        "        if best_candidate:\n",
        "            selected_features.append(best_candidate)\n",
        "            print(f\"\\nFeature set {selected_features} was best, accuracy is {best_score:.1f}%\\n\")\n",
        "        iteration += 1\n",
        "\n",
        "\n",
        "    print(f\"Finished search!! The best feature subset is {selected_features}, which has an accuracy of {best_score:.1f}%\")\n",
        "\n",
        "# Backward Elimination with detailed trace\n",
        "\n",
        "#Feature selection technique that iteratively removes the worst(least significant feature) to improve model\n",
        "def backward_elimination(all_features):\n",
        "  #initialize with all features and evaluate accuracy\n",
        "    selected_features = list(all_features)\n",
        "    best_score = evaluate(selected_features)\n",
        "\n",
        "    print(\"\\nYou have selected Backward Elimination\\n\")\n",
        "    print(f\"Using all features and “random” evaluation, I get an accuracy of {best_score:.1f}%\\n\")\n",
        "\n",
        "    improvement = True\n",
        "    while improvement:\n",
        "        print(f\"Beginning search.\\n\")\n",
        "        #reset improvement flag, worst candidate to none and initialize candidate list\n",
        "        improvement = False\n",
        "        worst_candidate = None\n",
        "        candidates = []\n",
        "\n",
        "        #iterate over selected features, evaluate accuracy with candidate set and append the score to the list\n",
        "        for feature in selected_features:\n",
        "            candidate_features = [f for f in selected_features if f != feature]\n",
        "            score = evaluate(candidate_features)\n",
        "            candidates.append((feature, score))\n",
        "\n",
        "            #iterate over features and their scores,\n",
        "        for feature, score in candidates:\n",
        "            print(f\"Using feature(s) {[f for f in selected_features if f != feature]} accuracy is {score:.1f}%\")\n",
        "            #if current candidate improves score, update best score, update worst candidate and set improvement flag\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                worst_candidate = feature\n",
        "                improvement = True\n",
        "\n",
        "        #if there is an improvement, remove the worst candidate feature\n",
        "        if worst_candidate:\n",
        "            selected_features.remove(worst_candidate)\n",
        "            print(f\"\\nFeature set {selected_features} was best, accuracy is {best_score:.1f}%\\n\")\n",
        "\n",
        "\n",
        "    print(f\"The best feature subset is {selected_features}, which has an accuracy of {best_score:.1f}%\")\n",
        "\n",
        "# Main program to ask user for input and run the chosen algorithm\n",
        "def main():\n",
        "    all_features = ['1', '2', '3', '4']\n",
        "\n",
        "    print(\"Welcome to Our CS170 Group's Feature Selection Algorithm.\")\n",
        "    print(f\"Please enter total number of features: {len(all_features)}\")\n",
        "    print(\"Type the number of the algorithm you want to run.\")\n",
        "    print(\"\\n1. Forward Selection\\n2. Backward Elimination\\n\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1 for Forward Selection, 2 for Backward Elimination): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        forward_selection(all_features)\n",
        "    elif choice == '2':\n",
        "        backward_elimination(all_features)\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojgeGSm8-jIU",
        "outputId": "d6500648-118a-45d9-eba6-084344cd872c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Our CS170 Group's Feature Selection Algorithm.\n",
            "Please enter total number of features: 4\n",
            "Type the number of the algorithm you want to run.\n",
            "\n",
            "1. Forward Selection\n",
            "2. Backward Elimination\n",
            "\n",
            "Enter your choice (1 for Forward Selection, 2 for Backward Elimination): 1\n",
            "\n",
            "You have selected Forward Selection\n",
            "\n",
            "Using no features and “random” evaluation, I get an accuracy of 54.8%\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) ['1'] accuracy is 44.4%\n",
            "Using feature(s) ['2'] accuracy is 55.3%\n",
            "Using feature(s) ['3'] accuracy is 55.9%\n",
            "Using feature(s) ['4'] accuracy is 31.0%\n",
            "\n",
            "Feature set ['3'] was best, accuracy is 55.9%\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) ['3', '1'] accuracy is 64.0%\n",
            "Using feature(s) ['3', '2'] accuracy is 74.0%\n",
            "Using feature(s) ['3', '4'] accuracy is 32.8%\n",
            "\n",
            "Feature set ['3', '2'] was best, accuracy is 74.0%\n",
            "\n",
            "Beginning search.\n",
            "\n",
            "Using feature(s) ['3', '2', '1'] accuracy is 71.7%\n",
            "Using feature(s) ['3', '2', '4'] accuracy is 64.9%\n",
            "Finished search!! The best feature subset is ['3', '2'], which has an accuracy of 74.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "class NNClassifier:\n",
        "    def __init__(self):\n",
        "        self.training_data = None\n",
        "        self.training_labels = None\n",
        "\n",
        "    def train(self, training_data, training_labels):\n",
        "        \"\"\"Store the training data and labels.\"\"\"\n",
        "        self.training_data = training_data\n",
        "        self.training_labels = training_labels\n",
        "\n",
        "    def test(self, test_instance):\n",
        "        \"\"\"Return the class label of the nearest training instance.\"\"\"\n",
        "        if self.training_data is None or self.training_labels is None:\n",
        "            raise ValueError(\"The classifier has not been trained yet.\")\n",
        "\n",
        "        # Calculate Euclidean distances from the test instance to all training instances\n",
        "        distances = distance.cdist([test_instance], self.training_data, 'euclidean')\n",
        "        nearest_index = np.argmin(distances)\n",
        "        return self.training_labels[nearest_index]\n",
        "\n",
        "    def validate(self, feature_subset, data, labels):\n",
        "        \"\"\"Calculate the accuracy of the classifier given a specific feature subset.\"\"\"\n",
        "        if data is None or labels is None:\n",
        "            print(\"Unable to validate: Data or labels are missing.\")\n",
        "            return 0.0\n",
        "\n",
        "        selected_data = data[:, feature_subset]\n",
        "        num_instances = selected_data.shape[0]\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for i in range(num_instances):\n",
        "            # Leave-one-out validation: Train on all instances except the i-th one and test on the i-th one\n",
        "            train_data = np.delete(selected_data, i, axis=0)\n",
        "            train_labels = np.delete(labels, i, axis=0)\n",
        "            test_instance = selected_data[i]\n",
        "            true_label = labels[i]\n",
        "\n",
        "            self.train(train_data, train_labels)\n",
        "            predicted_label = self.test(test_instance)\n",
        "\n",
        "            if predicted_label == true_label:\n",
        "                correct_predictions += 1\n",
        "\n",
        "        accuracy = (correct_predictions / num_instances) * 100\n",
        "        return accuracy\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        # Load the entire dataset\n",
        "        data = np.genfromtxt(file_path)\n",
        "\n",
        "        # Extract labels and features separately\n",
        "        labels = data[:, 0].astype(int)\n",
        "        features = data[:, 1:]\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset from {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load small test dataset\n",
        "    small_dataset_path = '/content/small-test-dataset.txt'\n",
        "    data, labels = load_dataset(small_dataset_path)\n",
        "    feature_subset = [2, 4, 6]  # zero-indexed for features {3, 5, 7}\n",
        "\n",
        "    classifier = NNClassifier()\n",
        "    accuracy = classifier.validate(feature_subset, data, labels)\n",
        "\n",
        "    print(f\"Accuracy of NN classifier with feature subset {feature_subset} on small dataset: {accuracy:.2f}%\")\n",
        "\n",
        "    # Load large test dataset\n",
        "    large_dataset_path = '/content/large-test-dataset.txt'\n",
        "    data, labels = load_dataset(large_dataset_path)\n",
        "    feature_subset = [0, 14, 26]  # zero-indexed for features {1, 15, 27}\n",
        "\n",
        "    accuracy = classifier.validate(feature_subset, data, labels)\n",
        "\n",
        "    print(f\"Accuracy of NN classifier with feature subset {feature_subset} on large dataset: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urDtFjtkOBHd",
        "outputId": "3a4593c6-fdc8-4dcd-c599-804c1d294ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of NN classifier with feature subset [2, 4, 6] on small dataset: 89.00%\n",
            "Accuracy of NN classifier with feature subset [0, 14, 26] on large dataset: 94.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "class NNClassifier:\n",
        "    def __init__(self):\n",
        "        self.training_data = None\n",
        "        self.training_labels = None\n",
        "\n",
        "    def train(self, training_data, training_labels):\n",
        "        \"\"\"Store the training data and labels.\"\"\"\n",
        "        self.training_data = training_data\n",
        "        self.training_labels = training_labels\n",
        "\n",
        "    def test(self, test_instance):\n",
        "        \"\"\"Return the class label of the nearest training instance.\"\"\"\n",
        "        if self.training_data is None or self.training_labels is None:\n",
        "            raise ValueError(\"The classifier has not been trained yet.\")\n",
        "\n",
        "        # Calculate Euclidean distances from the test instance to all training instances\n",
        "        distances = distance.cdist([test_instance], self.training_data, 'euclidean')\n",
        "        nearest_index = np.argmin(distances)\n",
        "        return self.training_labels[nearest_index]\n",
        "\n",
        "    def validate(self, feature_subset, data, labels):\n",
        "        \"\"\"Calculate the accuracy of the classifier given a specific feature subset.\"\"\"\n",
        "        if data is None or labels is None:\n",
        "            print(\"Unable to validate: Data or labels are missing.\")\n",
        "            return 0.0\n",
        "\n",
        "        selected_data = data[:, feature_subset]\n",
        "        num_instances = selected_data.shape[0]\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for i in range(num_instances):\n",
        "            # Leave-one-out validation: Train on all instances except the i-th one and test on the i-th one\n",
        "            train_data = np.delete(selected_data, i, axis=0)\n",
        "            train_labels = np.delete(labels, i, axis=0)\n",
        "            test_instance = selected_data[i]\n",
        "            true_label = labels[i]\n",
        "\n",
        "            self.train(train_data, train_labels)\n",
        "            predicted_label = self.test(test_instance)\n",
        "\n",
        "            if predicted_label == true_label:\n",
        "                correct_predictions += 1\n",
        "\n",
        "        accuracy = (correct_predictions / num_instances) * 100\n",
        "        return accuracy\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        # Load the entire dataset\n",
        "        data = np.genfromtxt(file_path)\n",
        "\n",
        "        # Extract labels and features separately\n",
        "        labels = data[:, 0].astype(int)\n",
        "        features = data[:, 1:]\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset from {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Updated evaluation function using NNClassifier\n",
        "def evaluate(feature_subset, features, labels):\n",
        "    classifier = NNClassifier()\n",
        "    classifier.train(features, labels)  # Initialize the classifier with training data and labels\n",
        "    return classifier.validate(feature_subset, features, labels)\n",
        "\n",
        "\n",
        "# Forward Selection with detailed trace\n",
        "def forward_selection(all_features, features, labels):\n",
        "    selected_features = []\n",
        "    best_score = 0\n",
        "    improvement = True\n",
        "\n",
        "    print(\"\\nYou have selected Forward Selection\\n\")\n",
        "\n",
        "    # Initial accuracy with no features\n",
        "    initial_score = evaluate([], features, labels)\n",
        "    print(f\"Using no features and evaluation, I get an accuracy of {initial_score:.1f}%\\n\")\n",
        "\n",
        "    while improvement:\n",
        "        print(\"Beginning search.\\n\")\n",
        "        improvement = False\n",
        "        best_candidate = None\n",
        "        candidates = []\n",
        "        for feature in all_features:\n",
        "            if feature not in selected_features:\n",
        "                candidate_features = selected_features + [feature]\n",
        "                score = evaluate(candidate_features, features, labels)\n",
        "                candidates.append((feature, score))\n",
        "\n",
        "        for feature, score in candidates:\n",
        "            print(f\"Using feature(s) {selected_features + [feature]} accuracy is {score:.1f}%\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_candidate = feature\n",
        "                improvement = True\n",
        "\n",
        "        if best_candidate:\n",
        "            selected_features.append(best_candidate)\n",
        "            print(f\"\\nFeature set {selected_features} was best, accuracy is {best_score:.1f}%\\n\")\n",
        "\n",
        "    print(f\"Finished search!! The best feature subset is {selected_features}, which has an accuracy of {best_score:.1f}%\")\n",
        "\n",
        "# Backward Elimination with detailed trace\n",
        "def backward_elimination(all_features, features, labels):\n",
        "    selected_features = list(all_features)\n",
        "    best_score = evaluate(selected_features, features, labels)\n",
        "\n",
        "    print(\"\\nYou have selected Backward Elimination\\n\")\n",
        "    print(f\"Using all features and evaluation, I get an accuracy of {best_score:.1f}%\\n\")\n",
        "\n",
        "    improvement = True\n",
        "    while improvement:\n",
        "        print(\"Beginning search.\\n\")\n",
        "        improvement = False\n",
        "        worst_candidate = None\n",
        "        candidates = []\n",
        "\n",
        "        for feature in selected_features:\n",
        "            candidate_features = [f for f in selected_features if f != feature]\n",
        "            score = evaluate(candidate_features, features, labels)\n",
        "            candidates.append((feature, score))\n",
        "\n",
        "        for feature, score in candidates:\n",
        "            print(f\"Using feature(s) {[f for f in selected_features if f != feature]} accuracy is {score:.1f}%\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                worst_candidate = feature\n",
        "                improvement = True\n",
        "\n",
        "        if worst_candidate:\n",
        "            selected_features.remove(worst_candidate)\n",
        "            print(f\"\\nFeature set {selected_features} was best, accuracy is {best_score:.1f}%\\n\")\n",
        "\n",
        "    print(f\"The best feature subset is {selected_features}, which has an accuracy of {best_score:.1f}%\")\n",
        "\n",
        "# Main program to ask user for input and run the chosen algorithm\n",
        "def main():\n",
        "    small_dataset_path = '/content/CS170_Spring_2024_Small_data__63.txt'  # Replace with your small dataset path\n",
        "    large_dataset_path = '/content/CS170_Spring_2024_Large_data__63.txt'  # Replace with your large dataset path\n",
        "\n",
        "    features1, labels1 = load_dataset(small_dataset_path)\n",
        "    features2, labels2 = load_dataset(large_dataset_path)\n",
        "\n",
        "    if features1 is None or labels1 is None or features2 is None or labels2 is None:\n",
        "        return\n",
        "\n",
        "    print(\"Welcome to Our CS170 Group's Feature Selection Algorithm.\")\n",
        "    print(\"Please select the dataset you want to use:\")\n",
        "    print(\"1. Small Dataset\")\n",
        "    print(\"2. Large Dataset\")\n",
        "\n",
        "    dataset_choice = input(\"Enter your choice (1 for Small Dataset, 2 for Large Dataset): \").strip()\n",
        "\n",
        "    if dataset_choice == '1':\n",
        "        features = features1\n",
        "        labels = labels1\n",
        "    elif dataset_choice == '2':\n",
        "        features = features2\n",
        "        labels = labels2\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
        "        return\n",
        "\n",
        "    all_features = list(range(features.shape[1]))\n",
        "\n",
        "    print(f\"Please enter total number of features: {len(all_features)}\")\n",
        "    print(\"Type the number of the algorithm you want to run.\")\n",
        "    print(\"\\n1. Forward Selection\\n2. Backward Elimination\\n\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1 for Forward Selection, 2 for Backward Elimination): \").strip()\n",
        "\n",
        "    if choice == '1':\n",
        "        forward_selection(all_features, features, labels)\n",
        "    elif choice == '2':\n",
        "        backward_elimination(all_features, features, labels)\n",
        "    else:\n",
        "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OqqDCaUhdX4",
        "outputId": "8b4666b3-c23b-4a80-ddb2-6464760b5d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Our CS170 Group's Feature Selection Algorithm.\n",
            "Please select the dataset you want to use:\n",
            "1. Small Dataset\n",
            "2. Large Dataset\n",
            "Enter your choice (1 for Small Dataset, 2 for Large Dataset): 1\n",
            "Please enter total number of features: 10\n",
            "Type the number of the algorithm you want to run.\n",
            "\n",
            "1. Forward Selection\n",
            "2. Backward Elimination\n",
            "\n"
          ]
        }
      ]
    }
  ]
}